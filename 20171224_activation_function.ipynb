{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 活性化関数まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 活性化関数(activation function)の比較\n",
    "\n",
    "・Linear<br>\n",
    "・softplus<br>\n",
    "・sigmoid<br>\n",
    "・ReLU (Rectified Linear Unit)<br>\n",
    "・Leaky ReLU<br>\n",
    "・PReLU (Parametrized ReLU)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"04_actfunc/act03.png\" height=２350px width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・Linearは活性化関数をかけない状態 つまり線形分類器<br>\n",
    "   そのまま(線形分類器)だと、精度が全然出ていない<br>\n",
    "　 非線形性がニューラルネットに重要ということがわかる<br>\n",
    "・ReLU系強い。そしてsoft plusもなかなか健闘している。<br>\n",
    "　 両者の関数系は似ている<br>\n",
    "・PReLUが、validation accuracyまで見るともっとも優秀<br>\n",
    "　 意外にLeaky ReLUが力を発揮しきれていない？<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"04_actfunc/act04.png\" height=650px width=650px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLUの良さ\n",
    "\n",
    "・$max(0,x)$ は単純ゆえに早い<br>\n",
    "・0を作る → スパース性につながる<br>\n",
    "・$x>0$ の部分では微分値が常に1であるため勾配消失の心配はない<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考：線形関数をgoogle play ground で確認する\n",
    "\n",
    "google play ground で確認する　→\n",
    "<a href=\"https://goo.gl/5zbbO3\"> http://playground.tensorflow.org/#activation=linear</a>\n",
    "\n",
    "<img src=\"04_actfunc/act01_linear.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLUをgoogle play ground で確認する\n",
    "\n",
    "google play ground で確認する　→\n",
    "<a href=\"https://goo.gl/dtbHZV\"> http://playground.tensorflow.org/#activation=relu</a>\n",
    "<img src=\"04_actfunc/act02_relu.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLUのパラメータと精度\n",
    "\n",
    "・trainingデータの精度的にはaは小さい方がいい<br>\n",
    "　 → これは正解データに対して当てはめているだけ、実際は過学習している可能性がある。\n",
    "  \n",
    "・大事なのはvalidation accuracyで、実際にそのepochでtrainingしなかったものに対して<br>\n",
    "どのくらいの効果を発揮するか。\n",
    "\n",
    "・validation accuracyを見ると、0.0≤a<0.4 あたりで Leaky ReLUを調節すればよさそう。\n",
    "\n",
    "だが、大幅な精度改善というわけにはいかない\n",
    "\n",
    "今回は層が一層だから差があまりないのかも。深いそうになってくるとPReLUやLeaky ReLUが過学習を防ぐ方向に活躍してくれるらしい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"04_actfunc/act05.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras　の LeakyReLU 実装\n",
    "\n",
    "keras.layers.advanced_activations.LeakyReLU(alpha=0.3)<br>\n",
    "ユニットがアクティブでないときに微少な勾配を可能とするRectified Linear Unit<br>\n",
    "の特別なバージョン<br>\n",
    "\n",
    "$f = \\begin{cases}\n",
    "    x & (x>0) \n",
    "    \\\\\n",
    "     \\alpha * x  & (x<=0)\n",
    "  \\end{cases}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RReLU(Randomized ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "精度向上に貢献できそうなのが、Randomized ReLU。 \n",
    "\n",
    "違いは、Leaky ReLUの傾きにランダム性を付与することで、傾きに幅を与えている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"04_actfunc/act06.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Randomized Leaky Rectified Linear is the randomized version of leaky ReLU.\n",
    "It is first proposed and used in Kaggle NDSB Competition.\n",
    "The highlight of RReLU is that in training process, a_ji is a random number\n",
    "sampled from a uniform distribution U(l, u). Formally,\n",
    "we have:\n",
    "</pre>\n",
    "\n",
    "$y_{ji} = \\begin{cases}\n",
    "    x_{ji} & (x_{ji}>0)\n",
    "    \\\\\n",
    "     a_{ji}x_{ji}  & (x\\leq0)\n",
    "  \\end{cases}\n",
    "$\n",
    "\n",
    "(3)\n",
    "where\n",
    "$a_{ji} \\sim U(l, u)$, $l < u$ and $l, u \\in [0, 1) (4)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. activation functionの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "activation function\n",
    "'''\n",
    "def sigmoid(x):\n",
    "    return 1. / (1 + numpy.exp(-x))\n",
    "\n",
    "def dsigmoid(x):\n",
    "    return x * (1. - x)\n",
    "\n",
    "def tanh(x):\n",
    "    return numpy.tanh(x)\n",
    "\n",
    "def dtanh(x):\n",
    "    return 1. - x * x\n",
    "\n",
    "def softmax(x):\n",
    "    e = numpy.exp(x - numpy.max(x))  # prevent overflow\n",
    "    if e.ndim == 1:\n",
    "        return e / numpy.sum(e, axis=0)\n",
    "    else:  \n",
    "        return e / numpy.array([numpy.sum(e, axis=1)]).T  # ndim = 2\n",
    "\n",
    "def ReLU(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def dReLU(x):\n",
    "    return 1. * (x > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "\n",
    "keras - Advanced Activationsレイヤー<br>\n",
    "https://keras.io/ja/layers/advanced-activations/\n",
    "\n",
    "Empirical Evaluation of Rectified Activations in Convolution Network<br>\n",
    "https://arxiv.org/pdf/1505.00853.pdf<br>\n",
    "→Randomized ReLU　の元論文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
