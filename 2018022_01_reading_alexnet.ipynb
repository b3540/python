{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 論文を読む際に注意すべき観点\n",
    "\n",
    "- どんなもの？\n",
    "\n",
    "2012年の画像認識コンペILSVRCでDeep Neural Networkを使い圧勝し、第3次AIブームの火付け役\n",
    "となった。\n",
    "特徴量設計も機械学習で出来ることを立証した。\n",
    "\n",
    "- 先行研究と比べてどこがすごい？\n",
    "\n",
    "基本は後続のDeep Neural Networkでも広く採用されている<br>\n",
    "　・ ReLU活性化関数<br>\n",
    "　・ マルチGPUでの学習<br>\n",
    "　・ Data augmentation(データ拡張)<br>\n",
    "　・ Dropout<br>\n",
    "を採用し、8層で学習し大きな成果を上げた点。\n",
    "ImageNetのデータを元にしたILSVRCコンテストの大量データでそれを証明した。\n",
    "\n",
    "- 技術や手法のキモはどこ？\n",
    "\n",
    "手法に新たなブレイクスルーはないが、先行研究の成果を取り込み、成果を出して見せた点が重要。\n",
    "Random crop、水平反転、RGBチャネル強度をランダムに変化といったデータ拡張や、ドロップアウトで過学習を\n",
    "抑え、ReLUの採用で学習スピードを従来の活性化関数より　６倍高速化したことで、効率的な学習で\n",
    "大きなデータセットで成果を出している。\n",
    "\n",
    "- どうやって有効だと検証した？\n",
    "\n",
    "中間層を除くと2%程度精度が悪化することを確認していたり、ドロップアウトの有無などでエラー率が改善するかを\n",
    "検証したり、活性化関数を替えて学習速度の比較を実施したりしている。\n",
    "\n",
    "- 議論はある？\n",
    "\n",
    "階層の深さがどう精度に影響を与えるのか。\n",
    "\n",
    "- 次に読むべき論文は？\n",
    "\n",
    "GoogleNet, VGGNet あたり。\n",
    "\n",
    "[8] VGG‒Net (2014)\n",
    "- 一般物体認識⽤で19層のアーキテクチャへ\n",
    "- 小さい畳み込みサイズ(3x3)を多段にした\n",
    "\n",
    "K. Simonyan, A. Zisserman.<br>\n",
    "Very Deep Convolutional Networks for Large‒Scale Visual Recognition. arXiv: 1409.1556, 2014. <br>\n",
    "https://arxiv.org/pdf/1409.1556.pdf\n",
    "\n",
    "[9] GoogLeNet / Inception (2014~2015)\n",
    "- 22層のアーキテクチャ\n",
    "- auxiliary classiﬁers, Inception module GoogLeNet\n",
    "\n",
    "C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich.<br>\n",
    "Going deeper with convolutions. arXiv: 1409.4842, 2014. <br>\n",
    "https://arxiv.org/pdf/1409.4842.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CNN アーキテクチャの変遷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"15_imgnet/img08.png\" width=\"450px\" height=\"450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. 畳み込み型ネットの発⾒\n",
    "[1] Neocognitron  (1980)\n",
    "\n",
    "CNNのアーキテクチャの変遷  –畳み込み型ネットの発見\n",
    "- 局所的な結合というアイディア\n",
    "- 2種類（特徴抽出と情報集約）の処理を繰り返す\n",
    "\n",
    "[1]  K.  Fukushima.<br>\n",
    "Neocognitron: A self‒organizing neural network model for a mechanism of pattern recognition unaﬀected \n",
    "by shift in position. Biological  Cybernetics  36,  1980.<br>\n",
    "http://www.cs.princeton.edu/courses/archive/spr08/cos598B/Readings/Fukushima1980.pdf\n",
    "\n",
    "[2] LeNet  (1998)\n",
    "\n",
    "- 畳み込みとプーリング（サブサンプリング）の形に\n",
    "- Back  Propagation(BP)  によって学習\n",
    "\n",
    "[2] Y LeCun, L Bottou, Y Bengio, P Haﬀner.<br>\n",
    "Gradient‒based learning applied to document recognition.\n",
    "Proceedings  of  the  IEEE  86,  1998.<br>\n",
    "http://deeplearning.net/tutorial/lenet.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. プーリング,活性化関数,正則化\n",
    "\n",
    "[3] Ave./Max  Pooling,  Local  Contrast  Normalization  (2009)\n",
    "AlexNet22は，2012年のILSVRCにおいて，従来の画像認識のデファクトスタンダードであった\n",
    "SIFT + Fisher Vector + SVM5というアプローチに大差をつけて優勝し，\n",
    "一躍深層学習の有効性を知らしめたモデル．\n",
    "\n",
    "<img src=\"15_imgnet/img09.png\" width=\"450px\" height=\"450px\">\n",
    "\n",
    "上図はAlexNetのアーキテクチャ。\n",
    "\n",
    "・構造<br>\n",
    "AlexNetは，畳み込み層を5層重ねつつ，pooling層で特徴マップを縮小し，その後，3層の全結合層により最終的な出力を得る．基本的なアーキテクチャの設計思想はNeocognitronやLeNetを踏襲している．\n",
    "\n",
    "・分割学習<br>\n",
    "学習時には，当時のGPUのメモリ制約から，各層の特徴マップをチャネル方向に分割し，2台のGPUで独立して学習するというアプローチが取られた．幾つかの畳み込み層および全結合層では，より有効な特徴を学習するため，もう1台のGPUが担当しているチャネルも入力として利用している．\n",
    "\n",
    "・重みの初期化と最適化\n",
    "CNNの重みはガウス分布に従う乱数により初期化され，モーメンタム付きの確率的勾配降下法 (Stochastic Gradient Descent, SGD) により最適化が行われる．\n",
    "各パラメータはweigtht decay（L2正則化）により正則化が行われている．\n",
    "ロスが低下しなくなったタイミングで学習率を1/10に減少させることも行われており，上記の最適化の手法は，\n",
    "現在おいてもベストプラクティスとして利用されている．\n",
    "\n",
    "- 非CNN系画像認識のアイディアを導⼊\n",
    "\n",
    "[3] K. Jarrett, K. Kavukcuoglu, M. Ranzato, Y. LeCun.<br>\n",
    "What is the best multi‒stage architecture for object recognition?.  CVPR,  2009.<br>\n",
    "http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf\n",
    " \n",
    "[4] ReLU  (2011) \n",
    "- 活性化関数を単純に\n",
    "\n",
    "[4] X. Glorot, A. Bordes, Y. Bengio. Deep Sparse Rectiﬁer Neural Networks. AISTATS 11, 2011.<br>\n",
    "http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf\n",
    "\n",
    "- Dropout\n",
    "\n",
    "Dropoutは，学習時のネットワークについて，隠れ層のニューロンを一定確率で無効化する手法．\n",
    "これにより，擬似的に毎回異なるアーキテクチャで学習を行うこととなり，アンサンブル学習と同様の\n",
    "効果をもたらし，より汎化されたモデルを学習することができる．\n",
    "\n",
    "AlexNetでは，最初の2つの全結合層にこのdropoutが導入されている．\n",
    "\n",
    "Dropoutを行わない場合にはかなりの過学習が発生したが，dropoutによりこの過学習を抑えられる一方，\n",
    "収束までのステップ数が約2倍になったと報告されている．\n",
    "\n",
    "[5] Dropout  (2012) \n",
    "- 過学習を防ぐための正則化技術の導⼊\n",
    "\n",
    "[5] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, R. R. Salakhutdinov. Improving neural networks  by  preventing co‒adaptation of feature detectors.  arXiv:  1207.0580,  2012.<br>\n",
    "https://arxiv.org/pdf/1207.0580.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. 畳み込みの多層化と複雑化\n",
    "\n",
    "[6] AlexNet  (2012) \n",
    "\n",
    "- 大規模一般物体認識での成功\n",
    "- Data  Augmentationとこれまでの要素技術の結集  (8層) \n",
    "\n",
    "[6] A. Krizhevsky, I. Sutskever, G. E. Hinton.<br>\n",
    "ImageNet Classiﬁcation with Deep Convolutional Neural Networks.  NIPS,  2012.<br>\n",
    "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n",
    "\n",
    "[7] Network  in  Network(2013)\n",
    "\n",
    "- 畳み込み層に非線形性を導⼊\n",
    "- 全結合部を使わないという提案  (global  ave.  pooling)\n",
    "\n",
    "[7] M. Lin, Q. Chen, S. Yan.<br>\n",
    "Network In Network. arXiv:  1312.4400,  2013.<br>\n",
    "https://arxiv.org/pdf/1312.4400.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4. 畳み込みの多層化と複雑化\n",
    "\n",
    "[8] VGG‒Net  (2014)\n",
    "- 一般物体認識⽤で**19層**のアーキテクチャへ\n",
    "- 小さい畳み込みサイズ(3x3)を多段にした\n",
    "\n",
    "[8] K. Simonyan, A. Zisserman.<br>\n",
    "Very Deep Convolutional Networks for Large‒Scale Visual Recognition.  arXiv:  1409.1556,  2014. <br>\n",
    "https://arxiv.org/pdf/1409.1556.pdf\n",
    "\n",
    "[9][10] GoogLeNet / Inception  (2014~2015) \n",
    "\n",
    "- **22層**のアーキテクチャ\n",
    "- auxiliary  classiﬁers, Inception module GoogLeNet\n",
    "\n",
    "[9] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich.<br>\n",
    "Going  deeper with convolutions.  arXiv:  1409.4842,  2014. <br>\n",
    "https://arxiv.org/pdf/1409.4842.pdf\n",
    "\n",
    "[10]  C.  Szegedy,  V.  Vanhoucke,  S.  Ioﬀe,  J.  Shlens,  Z.  Wojna.<br>\n",
    "Rethinking the Inception Architecture for Computer Vision.  arXiv:  1512.00567,  2015.<br>\n",
    "https://arxiv.org/pdf/1512.00567.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-5. アーキテクチャの多様化\n",
    "\n",
    "[11] SPP‒Net  (2014)\n",
    "\n",
    "- 様々なサイズの入力画像を許容\n",
    "- CNN⼊⼒時のリサイズを回避\n",
    "\n",
    "[11] K. He, X. Zhang, S. Ren, J. Sun.<br>\n",
    "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition. arXiv:  1406.4729, 2014.<br>\n",
    "https://arxiv.org/pdf/1406.4729.pdf\n",
    "    \n",
    "[12] All Convolutional Net, guided BP (2014)\n",
    "\n",
    "- プーリングをストライド2の畳み込みに置き換える\n",
    "- guided  BPによる超高次層の特徴可視化\n",
    "\n",
    "[12] J. T. Springenberg, A. Dosovitskiy, T. Brox, M. Riedmiller.<br>\n",
    "Striving for Simplicity: The All Convolutional Net.  arXiv:  1412.6806,  2014.<br>\n",
    "https://arxiv.org/pdf/1412.6806.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-6. 学習方法の多様化\n",
    "\n",
    "[13] Exemplar  CNN  (2014)\n",
    "\n",
    "- Data  Augmentation  を利用して教師なし表現学習\n",
    "\n",
    "[13] A. Dosovitskiy, P. Fischer, J. T. Springenberg, M. Riedmiller, T. Brox.\n",
    "Discriminative Unsupervised Feature Learning　with Exemplar Convolutional Neural Networks.\n",
    "arXiv:  1406.6909, 2014. <br>\n",
    "https://arxiv.org/pdf/1406.6909.pdf\n",
    "\n",
    "\n",
    "[14] Triplet  Network  (2014) \n",
    "\n",
    "- ユークリッド空間上でCNN上の特徴同⼠士が, 同クラスなら近くなるように,  別クラスなら遠くなるように\n",
    "\n",
    "[14] E. Hoﬀer, N. Ailon.\n",
    "Deep metric learning using Triplet network.  arXiv:  1412.6622,  2014.<br>\n",
    "https://arxiv.org/pdf/1412.6622.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-7. 超多層アーキテクチャへ\n",
    "\n",
    "[15] Batch  Normalization  (2015) \n",
    "\n",
    "- パラメータ付き正規化処理\n",
    "- 複雑なアーキテクチャをスクラッチで学習させる必須技術\n",
    "\n",
    "[15] S. Ioﬀe, C. Szegedy.<br>\n",
    "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.\n",
    "arXiv:  1502.03167,  2015.<br>\n",
    "https://arxiv.org/pdf/1502.03167.pdf\n",
    "\n",
    "[16] ResNet  (2015) \n",
    "\n",
    "- 152層からなる超多層アーキテクチャ\n",
    "- 途中の特徴マップを何層か先にバイパスしてやる\n",
    "\n",
    "[16] K. He, X. Zhang, S. Ren, J. Sun.<br>\n",
    "Deep Residual Learning for Image Recognition. arXiv:1512.03385,  2015.<br>\n",
    "https://arxiv.org/pdf/1512.03385.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-8. 確率的勾配降下法における学習率調整法\n",
    "\n",
    "[17] AdaGrad / [18] RMSProp / [19] AdaDelta / [20] Adam\n",
    "\n",
    "- 一概にどれが最も良いとは言えない  (AdaGrad以外は比較的優秀？) \n",
    "- データセットや問題によって適切なハイパーパラメータ が異なってくる\n",
    "\n",
    "[17]  J.  Duchi,  E.  Hazan,  Y.  Singer.<br>\n",
    "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.\n",
    "Journal  of  Machine  Learning  Research  12  ,2011.<br>\n",
    "http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf\n",
    "\n",
    "[18] T. Tieleman, G. Hinton.<br>\n",
    "Divide the gradient by a running average of its recent magnitude.\n",
    "COURSERA:  Neural  Networks  for  Machine  Learning  4,  2012. <br>\n",
    "https://www.coursera.org/learn/neural-networks/lecture/YQHki/rmsprop-divide-the-gradient-by-a-running-average-of-its-recent-magnitude\n",
    "\n",
    "[19] M. D. Zeiler.<br>\n",
    "ADADELTA: An Adaptive Learning Rate Method.  arXiv:  1212.5701,  2012.<br>\n",
    "https://arxiv.org/pdf/1212.5701.pdf\n",
    "\n",
    "[20] D. Kingma, J. Ba.<br>\n",
    "Adam: A Method for Stochastic Optimization.  arXiv:  1412.6980,  2014.<br>\n",
    "https://arxiv.org/pdf/1412.6980.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 論文精読"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ImageNet LSVRC-2010コンテストの120万の高解像度画像を<br>\n",
    "1000種類の異なる画像に分類するために、大きくて深い畳み込みニューラルネットワークを訓練した\n",
    "\n",
    "テストデータでは、過去最高レベルの37.5％と17.0％のエラー率を達成。\n",
    "\n",
    "6000万のパラメータと65万のニューロンを有するニューラルネットワークは、<br>\n",
    "5つの畳み込み層から成り、そのうちのいくつかには最大プール層、3つの完全に連結された層および<br>\n",
    "最終的な1000通りのソフトマックスで構成される。\n",
    "\n",
    "トレーニングを高速化するために、非飽和ニューロンと非常に効率的なコンボリューション演算のGPU実装を使用。\n",
    "\n",
    "完全に接続されたレイヤーのオーバーフィットを減らすために、 非常に効果的であることが判明している<br>\n",
    "\"ドロップアウト\"と呼ばれる正規化方法を採用。\n",
    "\n",
    "また、ILSVRC-2012の競技会でこのモデルの変種を入力し、第2位入賞で達成された26.2％"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 「saturating」飽和　ということ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "関数が飽和しているというのはx→±∞ の際にリミッターとなるある値で抑えられているということ。\n",
    "すなわち、微分すると勾配が大幅に小さくなる。\n",
    "\n",
    "非飽和というのは逆にリミッターがない状態なので、微分しても減衰しないので、\n",
    "階層が深くなっても勾配が消失する問題が発生しない。\n",
    "\n",
    "シグモイド関数や tanh関数だと飽和関数であり、勾配消失問題が内在していたが、\n",
    "ReLUだと非飽和関数であるのでこの問題が起こらず、学習スピードが向上するという結果がこの論文で\n",
    "示された。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "物体認識に対する現在のアプローチは、機械学習法を本質的に使用する。\n",
    "\n",
    "パフォーマンスを向上させるために、より大きなデータセットを収集し、より強力なモデルを学習し、<br>\n",
    "オーバーフィットを防止する技術を利用している。\n",
    "\n",
    "最近まで、ラベルづけされた画像データセットは比較的小さかった。<br>\n",
    "（例えば、NORB [16]、Caltech-101/256 [8,9]、およびCIFAR-10/100 [12]）。\n",
    "\n",
    "簡単な認識タスクは、特にラベル保存変換で拡張されている場合は、このサイズのデータセットで<br>\n",
    "非常にうまく解決できます。\n",
    "\n",
    "例えば、MNIST桁認識タスク（<0.3％）の現在のエラー率は人間のパフォーマンスに近づいている[4]。\n",
    "\n",
    "しかし、現実的な設定のオブジェクトはかなりの変動性を示します。したがって、<br>\n",
    "それらを認識するためには、より大きなトレーニングセットを使用する必要があります。\n",
    "\n",
    "小さな画像データセットの欠点が広く認識されている(Pintoら[21]）が、<br>\n",
    "最近では、数百万の画像を持つラベル付きデータセットを収集することが可能になっています。\n",
    "\n",
    "新しい大きなデータセットには、<br>\n",
    "数十万の完全にセグメント化された画像で構成されているLabelMe [23]や。<br>\n",
    "22,000以上のカテゴリで1500万以上の高解像度画像で構成されるImageNet [6]があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn about thousands of objects from millions of images, we need a model with a large learning\n",
    "capacity. However, the immense complexity of the object recognition task means that this problem\n",
    "cannot be specified even by a dataset as large as ImageNet, so our model should also have lots\n",
    "of prior knowledge to compensate for all the data we don’t have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数百万の画像から何千ものオブジェクトを学習するには、学習能力の高いモデルが必要です。\n",
    "\n",
    "しかし、オブジェクト認識タスクの莫大な複雑さは、**ImageNetほどのデータセットでも指定できない**こと<br>\n",
    "を意味します。\n",
    "\n",
    "したがって、我々のモデルは、我々が持っていないすべてのデータを補うための多くの事前知識も必要です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks(CNNs) constitute one such class of models [16, 11, 13, 18, 15, 22, 26]. \n",
    "\n",
    "Their capacity can be controlled by varying their depth and breadth, and they also make strong\n",
    "and mostly correct assumptions about the nature of images (namely, stationarity of statistics\n",
    "and locality of pixel dependencies).\n",
    "\n",
    "Thus, compared to standard feedforward neural networks with similarly-sized layers, CNNs have\n",
    "much fewer connections and parameters and so they are easier to train, while their theoretically-best\n",
    "performance is likely to be only slightly worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "畳み込みニューラルネットワーク（CNN）は、[16,11,13,18,15,22,26]のようなモデルのクラスを構成する。<br>\n",
    "\n",
    "CNN の性能は、その深さと幅を変えることによって制御することができ、また、画像の性質<br>\n",
    "（すなわち、統計の定常性およびピクセル依存性の局所性）について強く、ほとんど正しい仮定を行う。\n",
    "\n",
    "従って、同様のサイズの層を有する標準的なフィードフォワードニューラルネットワークと比較して、<br>\n",
    "CNNは接続数及びパラメータが非常に少なく、訓練がより用意であるが、<br>\n",
    "理論的に最良の性能よりほんのわずかに悪化する可能性がある程度である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNの魅力的な性質にもかかわらず、ローカルアーキテクチャの相対的な効率性にもかかわらず、<br>\n",
    "高解像度の画像に大規模に適用するにはまだまだ高価でした。\n",
    "\n",
    "幸いなことに、高度に最適化された2D畳み込みの実装と組み合わせた現在のGPUは、興味深い大規模な<br>\n",
    "CNNのトレーニングを容易にするほど強力ですが、ImageNetなどの最近のデータセットには、<br>\n",
    "過酷な過補正なしにモデルを訓練するのに十分なラベル付きの例が含まれています。\n",
    "\n",
    "この論文の具体的な貢献は、ILSVRC-2010とILSVRC-2012の競技会で使用されているImageNetのサブセットについて、最大の畳み込みニューラルネットワークの1つを訓練しました。これらのデータセット。\n",
    "\n",
    "私たちは2D畳み込みの高度に最適化されたGPU実装と、畳み込みニューラルネットワークの訓練に内在する他のすべての操作を公表しました。\n",
    "\n",
    "当社のネットワークには、第3章で詳述されているように、パフォーマンスを向上させ、トレーニング時間を短縮する、多くの新しく珍しい機能が含まれています。\n",
    "\n",
    "ネットワークのサイズは、120万のラベル付きトレーニング例でも重大な問題を上回りました。そこで、セクション4で説明しているオーバーフィット防止のためのいくつかの効果的な手法を使用しました。最終ネットワークには、5つの畳み込みレイヤーと3つの完全接続レイヤー、この深さは重要であるようです：\n",
    "畳み込みレイヤーを削除すると（それぞれがモデルのパラメーターの1％以下を含む）、パフォーマンスが低下します。\n",
    "\n",
    "結局、ネットワークのサイズは、現在のGPUで利用可能なメモリの量と、許容したいトレーニング時間の長さによって主に制限されます。\n",
    "\n",
    "当社のネットワークは、2つのGTX 580 3GB GPUでトレーニングするのに5日から6日かかります。私たちの実験では、より速いGPUとより大きいデータセットが利用可能になるのを待って、結果を改善できることを示唆しています。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ImageNetは、**およそ22,000カテゴリに属する​​1500万以上**の高解像度画像のデータセットです。\n",
    "\n",
    "画像はWebから収集され、AmazonのMechanical Turkのクラウドソーシングツールを使用して<br>\n",
    "人の手によってラベル付けされました。<br>\n",
    "2010年に始まったPascal Visual Object Challengeの一環として、ImageNet大規模視覚認識チャレンジ<br>\n",
    "と呼ばれる年次大会（ILSVRC）が開催されました。\n",
    "\n",
    "ILSVRCは、ImageNetのサブセットを使用して、1000のカテゴリのそれぞれにおよそ1000のイメージを使用します。<br>\n",
    "全体として、約120万のトレーニング画像、50,000の検証画像、150,000のテスト画像があります。\n",
    "\n",
    "ILSVRC-2010は、テストセットラベルが利用できる唯一のバージョンのILSVRCであり、\n",
    "我々の実験はほとんどこれに基づいています。\n",
    "\n",
    "私たちはILSVRC-2012の競技会でもモデルを入力したので、<br>\n",
    "第6章では、テストセットラベルが利用できないこのバージョンのデータセットについても結果を報告します。\n",
    "\n",
    "ImageNetでは、top-1およびtop-5の2つのエラー率を報告するのが通例です。<br>\n",
    "top-5エラー率は、モデルによって最も可能性が高いと考えられる5つのラベルのうち<br>\n",
    "正しいラベルがないテスト画像の割合です。\n",
    "\n",
    "ImageNetは可変解像度の画像で構成され、システムは一定の入力次元を必要とする。\n",
    "\n",
    "したがって、画像を256×256の固定解像度にダウンサンプリングしました。<br>\n",
    "矩形の画像が与えられたとき、最初に短辺の長さが256になるように画像を再スケーリングし、<br>\n",
    "中央256×256に切り取っています。\n",
    "\n",
    "各ピクセルからの訓練セット上の平均活動量を差し引く以外は、他の方法で画像を前処理せず、<br>\n",
    "中央の生のRGBピクセル値を利用しネットワークで訓練しました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "畳み込み層を5層重ねつつ，pooling層で特徴マップを縮小し，その後，3層の全結合層により最終的な出力を得る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 ReLU Nonlinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "従来，非線形な活性化関数としては，\n",
    "\n",
    "$f(x)=tanh(x)$  または <br>\n",
    "$f(x)=(1+e^{-x})^{-1}$\n",
    "\n",
    "である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rectified Linear Units を利用することで学習を高速化している．<br>\n",
    "これは，深いネットワークで従来の活性化関数を利用した場合に発生する勾配消失問題を<br>\n",
    "解決できるためである．\n",
    "\n",
    "ReLUは，その後改良がなされた活性化関数も提案されているが，最新のモデルでも標準的な<br>\n",
    "活性化関数として広く利用されている．\n",
    "\n",
    "ReLUを用いた深い畳み込みニューラルネットワークは、**tanh単位の等価物よりも数倍速い**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特定の4層畳み込みネットワークのCIFAR-10データセットで25％のトレーニングエラーに達するのに必要な反復の回数。\n",
    "\n",
    "図1\n",
    "<img src=\"15_imgnet/img01.png\" width=\"250px\" height=\"250px\">\n",
    "\n",
    "この図が示していることは、従来の飽和活性化い関数を使用した場合、このような大規模なニューラルネットワークを実験することができなかったということ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not the first to consider alternatives to traditional\n",
    "neuron models in CNNs. For example, Jarrett\n",
    "et al. [11] claim that the nonlinearity f(x) = |tanh(x)|\n",
    "works particularly well with their type of contrast normalization\n",
    "followed by local average pooling on the\n",
    "Caltech-101 dataset. However, on this dataset the primary\n",
    "concern is preventing overfitting, so the effect\n",
    "they are observing is different from the accelerated\n",
    "ability to fit the training set which we report when using\n",
    "ReLUs. Faster learning has a great influence on the\n",
    "performance of large models trained on large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNの伝統的なニューロンモデル＝活性化関数の代替案を最初に検討しているわけではない。\n",
    "\n",
    "例えば、Jarrett et al. [11] は、非線形性 $f（x）= |tanh（x）|$ が\n",
    "Caltech-101データセットのローカル平均プーリングに続いて、コントラストの正規化のタイプに特に適していると主張している。\n",
    "\n",
    "しかし、このデータセットでは、主な関心事は過適合を防止しているため、彼らが観察している効果は、ReLUsを使用するときに報告するトレーニングセットに合わせるための加速能力とは異なる。\n",
    "\n",
    "より速い学習は、大規模なデータセットで訓練された大型モデルのパフォーマンスに大きな影響を与える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークの線形変換と活性化関数について<br>\n",
    "http://s0sem0y.hatenablog.com/entry/2017/06/15/072248\n",
    "\n",
    "- 勾配消失問題\n",
    "\n",
    "入力が大きくなればなるほど、微分の値は小さくなっていきます。入力の値が仮に偶然１０などになった場合、微分の値が極めて小さな値を取ることが想定できます。この小さな値が学習の係数に掛かってきてしまうため、（本来はまだ最適解にたどり着いていないのに）学習が滞ってしまう勾配消失問題の引き金になってしまうのです。\n",
    "\n",
    "- ネットワークのスパース化\n",
    "\n",
    "ReLUのもう１つの特徴としては、ネットワークのスパース化に貢献するという点です。ニューラルネットワークがスパースであるとは、出力が０であるユニットがたくさんある状態を指します。\n",
    "\n",
    "仮にあるユニットへの入力が０未満になると、出力が０になります。そしてその領域において、ReLUの微分は０であるため、これが学習の係数に掛かるため、結果としてそのユニットは全く学習が行われなくなります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training on Multiple GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single GTX 580 GPU has only 3GB of memory, which limits the maximum size of the networks\n",
    "that can be trained on it. It turns out that 1.2 million training examples are enough to train networks\n",
    "which are too big to fit on one GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1つのGTX 580 GPUには3GBのメモリしか搭載されていない<br>\n",
    "→訓練できるネットワークの最大サイズが制限されている。\n",
    "\n",
    "1つのGPUに収まらないほど大きすぎるネットワークをトレーニングするには、120万のトレーニング例で十分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we spread the net across two GPUs. Current GPUs\n",
    "are particularly well-suited to cross-GPU parallelization, as they are able to read from and write to\n",
    "one another’s memory directly, without going through host machine memory. The parallelization\n",
    "scheme that we employ essentially puts half of the kernels (or neurons) on each GPU, with one\n",
    "additional trick: the GPUs communicate only in certain layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そのため、2つのGPUにネットを広げました。\n",
    "現在のGPUは、GPU間の並列化に特に適しています。これは、GPUの読み書きが可能なためです\n",
    "ホストマシンのメモリを介さずに、お互いのメモリを直接に使用できます。\n",
    "\n",
    "基本的に採用している並列化スキームは、各GPUにカーネル（またはニューロン）の半分を置きます。さらに1つのトリックがあります.GPUは特定のレイヤーでのみ通信します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that, for example, the\n",
    "kernels of layer 3 take input from all kernel maps in layer 2. However, kernels in layer 4 take input\n",
    "only from those kernel maps in layer 3 which reside on the same GPU. Choosing the pattern of\n",
    "connectivity is a problem for cross-validation, but this allows us to precisely tune the amount of\n",
    "communication until it is an acceptable fraction of the amount of computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "レイヤー3のカーネルは、レイヤー2のすべてのカーネルマップから入力を受け取る。\n",
    "\n",
    "レイヤ4のカーネルは、同じGPU上にあるレイヤ3のカーネルマップからのみ入力。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resultant architecture is somewhat similar to that of the “columnar” CNN employed by Cire¸san\n",
    "et al. [5], except that our columns are not independent (see Figure 2).\n",
    "\n",
    "This scheme reduces our top-1 and top-5 error rates by 1.7% and 1.2%, respectively, as compared with a net with half as many kernels in each convolutional layer trained on one GPU.\n",
    "\n",
    "The two-GPU net takes slightly less time to train than the one-GPU net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得られたアーキテクチャは、Cireandsan et al[5] によって採用された「円柱状」CNNのものと幾分類似している。\n",
    "\n",
    "ただし、列は独立していない（図2参照）。\n",
    "\n",
    "このスキームでは、1つのGPUで訓練された各畳み込みレイヤのカーネルの半分のネットを使用した場合と比較して、<br>\n",
    "**トップ1エラーとトップ5エラーがそれぞれ1.7％と1.2％減少します**。 \n",
    "\n",
    "2つのGPUネットは、1つのGPUネット2よりもトレーニングにかかる​​時間がわずかに短縮されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Local Response Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local Response Normalization (LRN) は，<br>\n",
    "**特徴マップの同一の位置にあり，隣接するチャネルの出力の値から，自身の出力の値を<br>\n",
    "正規化する手法**である．\n",
    "\n",
    "空間的に隣接する出力も考慮して正規化を行う Local Contrast Normalization (LCN)と比較して，<br>\n",
    "平均値を引く処理を行わず，より適切な正規化が行えるとしている．\n",
    "\n",
    "VGGNetでは効果が認められなかったことや，batch normalizationの登場により，<br>\n",
    "近年のモデルでは利用されなくなっている．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLUs have the desirable property that they do not require input normalization to prevent them\n",
    "from saturating.\n",
    "\n",
    "If at least some training examples produce a positive input to a ReLU, learning will\n",
    "happen in that neuron.\n",
    "\n",
    "However, we still find that the following local normalization scheme aids\n",
    "generalization.\n",
    "\n",
    "Denoting by $a^i_{x,y}$ the activity of a neuron computed by applying kernel $i$ at position\n",
    "(x, y) and then applying the ReLU nonlinearity, the response-normalized activity $b^i_{x,y}$ is given by\n",
    "the expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLUは、飽和を防ぐために**入力正規化を必要としない**。\n",
    "\n",
    "少なくともいくつかのトレーニング例がReLUへの正の入力を生成する場合、そのニューロンで学習が行われる。\n",
    "\n",
    "しかし、我々は次の局所的正規化スキームが一般化を助けることをまだ見いだしている。\n",
    "\n",
    "カーネル $i$ を位置（$x$, $y$）に適用することによって計算されるニューロンの活動を（$a ^ i_x, y$）で示し、\n",
    "ReLU非線形性を適用すると、応答正規化された活性化 $b ^ i_x, y$　は次の式で与えられます"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "図2\n",
    "<img src=\"15_imgnet/img02.png\" width=\"450px\" height=\"450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the sum runs over n “adjacent” kernel maps at the same spatial position, and N is the total\n",
    "number of kernels in the layer.\n",
    "\n",
    "この和は、同じ空間位置でn個の「隣接する」カーネルマップ上で実行され、Nは、層内のカーネルの総数である。\n",
    "\n",
    "The ordering of the kernel maps is of course arbitrary and determined\n",
    "before training begins. \n",
    "\n",
    "もちろん、カーネルマップの順序付けは、訓練が始まる前に任意で決定されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sort of response normalization implements a form of lateral inhibition\n",
    "inspired by the type found in real neurons, creating competition for big activities amongst neuron\n",
    "outputs computed using different kernels. \n",
    "\n",
    "このような応答の正規化は、実際のニューロンに見られるタイプに触発された一種の横方向抑制を実装し、異なるカーネルを使って計算されたニューロン出力の間の大きな活動のための競争を作り出します。\n",
    "\n",
    "The constants k, n, α, and β are hyper-parameters whose\n",
    "values are determined using a validation set; we used k = 2, n = 5, α = 10−4, and β = 0.75. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We applied this normalization after applying the ReLU nonlinearity in certain layers (see Section 3.5).\n",
    "\n",
    "特定の層でReLU非線形性を適用した後にこの正規化を適用した（3.5節参照）。\n",
    "\n",
    "This scheme bears some resemblance to the local contrast normalization scheme of Jarrett et al. [11],\n",
    "but ours would be more correctly termed “brightness normalization”, since we do not subtract the\n",
    "mean activity.\n",
    "\n",
    "Response normalization reduces our top-1 and top-5 error rates by 1.4% and 1.2%,\n",
    "respectively. <br>\n",
    "We also verified the effectiveness of this scheme on the CIFAR-10 dataset: a four-layer\n",
    "CNN achieved a 13% test error rate without normalization and 11% with normalization3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "空間的に隣接する出力も考慮して正規化を行う Local Contrast Normalization (LCN)[11]と比較して，<br>\n",
    "平均値を引く処理を行わず，より適切な正規化が行える．\n",
    "\n",
    "レスポンスの正規化により、トップ1とトップ5のエラー率はそれぞれ1.4％と1.2％低下。<br>\n",
    "また、CIFAR-10データセットに対するこのスキームの有効性も検証.<br>\n",
    "4層CNNは、正規化なしで13％のテストエラー率、正規化では11％のテストエラー率を達成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Overlapping Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling層は， $s$ ピクセルずつ離れたグリッドにおいて，周辺 $z$ ピクセルの値を<br>\n",
    "maxやaverage関数によって集約する処理と一般化することができる．\n",
    "\n",
    "通常，pooling層は $s=z$ とされ，集約されるピクセルが複数のグリッドにまたがって<br>\n",
    "overlapしないことが一般的であった．\n",
    "\n",
    "AlexNetでは， $s=2$, $z=3$ のmax poolingを利用しており，この場合，<br>\n",
    "集約されるピクセル領域がオーバラップすることになる．\n",
    "\n",
    "→このoverlapping poolingにより，過学習を低減し，わずかに最終的な精度が向上する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling layers in CNNs summarize the outputs of neighboring groups of neurons in the same kernel\n",
    "map.\n",
    "\n",
    "Traditionally, the neighborhoods summarized by adjacent pooling units do not overlap (e.g.,\n",
    "[17, 11, 4]).\n",
    "\n",
    "To be more precise, a pooling layer can be thought of as consisting of a grid of pooling\n",
    "units spaced s pixels apart, each summarizing a neighborhood of size z × z centered at the location\n",
    "of the pooling unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNのプーリング層は、同じカーネルマップ内のニューロンの隣接グループの出力を要約します。<br>\n",
    "伝統的に、隣接するプールユニットによって要約された近傍は、重複しない\n",
    "（例えば、[17,11,4]）。\n",
    "\n",
    "より正確に言えば、プーリング層は、間隔が s ピクセル離れたグリッドから構成され、<br>\n",
    "周辺　z ピクセルの集約処理をすると考えられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set s = z, we obtain traditional local pooling as commonly employed in CNNs. <br>\n",
    "If we set s < z, we obtain overlapping pooling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s = Z なら伝統的なプーリング層が得られ、s < z ならオーバーラップ　プーリング層が得られます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we use throughout our network, with s = 2 and z = 3.\n",
    "\n",
    "AlexNetでは，  $s=2,  z=3$  のmax poolingを利用しており，この場合，<br>\n",
    "集約されるピクセル領域がオーバラップすることになる．\n",
    "\n",
    "This scheme reduces the top-1 and top-5 error rates by 0.4% and 0.3%, respectively,\n",
    "as compared with the non-overlapping scheme s = 2, z = 2, which produces\n",
    "output of equivalent dimensions.\n",
    "\n",
    "オーバーラップしない構造に比べて、このプーリング構造により、トップ1とトップ5のエラー率はそれぞれ0.4％と0.3％低下します。\n",
    "\n",
    "We generally observe during training that models with overlapping pooling find it slightly more difficult to overfit.\n",
    "\n",
    "一般に重複しているプーリングを持つモデルは、過学習しにくい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Overall Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"15_imgnet/img09.png\" width=\"450px\" height=\"450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to describe the overall architecture of our CNN. As depicted in Figure 2, the net\n",
    "contains eight layers with weights; \n",
    "\n",
    "the first five are convolutional and the remaining three are fully-connected.\n",
    "\n",
    "The output of the last fully-connected layer is fed to a 1000-way softmax which produces\n",
    "a distribution over the 1000 class labels.\n",
    "\n",
    "Our network maximizes the multinomial logistic regression\n",
    "objective, which is equivalent to maximizing the average across training cases of the log-probability\n",
    "of the correct label under the prediction distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初の5つは畳み込み層で残りの3つは全結合層に接続。\n",
    "\n",
    "最後の全結合層の出力は、1000クラスのラベル上に分布を生成する softmax 関数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernels of the second, fourth, and fifth convolutional layers are connected only to those kernel\n",
    "maps in the previous layer which reside on the same GPU (see Figure 2). The kernels of the third\n",
    "convolutional layer are connected to all kernel maps in the second layer.\n",
    "\n",
    "The neurons in the fully-connected layers are connected to all neurons in the previous layer.\n",
    "\n",
    "Response-normalization layers follow the first and second convolutional layers.\n",
    "\n",
    "Max-pooling layers, of the kind described in Section 3.4, follow both response-normalization layers as well as the fifth convolutional layer.\n",
    "\n",
    "The ReLU non-linearity is applied to the output of every convolutional and fully-connected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第2,4,5の畳込み層のカーネルは、同じGPUに存在する前層のカーネルマップにのみ接続されます（図2参照）。\n",
    "\n",
    "第3の畳込み層のカーネルは、第2の層のすべてのカーネルマップに接続される。\n",
    "\n",
    "全結合層のニューロンは、前層のすべてのニューロンに接続されています。\n",
    "\n",
    "応答正規化層は、第1および第2の畳込み層に続く。\n",
    "\n",
    "「3.4 Overlapping Pooling」で説明したMax プーリング層では、応答正規化層と第5の畳込み層から接続します。\n",
    "\n",
    "ReLUの非線形性は、すべての畳込み層および全結合層の出力に適用されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first convolutional layer filters the 224×224×3 input image with 96 kernels of size 11×11×3\n",
    "with a stride of 4 pixels (this is the distance between the receptive field centers of neighboring\n",
    "neurons in a kernel map). \n",
    "\n",
    "The second convolutional layer takes as input the (response-normalized\n",
    "and pooled) output of the first convolutional layer and filters it with 256 kernels of size 5 × 5 × 48.\n",
    "The third, fourth, and fifth convolutional layers are connected to one another without any intervening\n",
    "pooling or normalization layers. \n",
    "\n",
    "The third convolutional layer has 384 kernels of size 3 × 3 ×\n",
    "256 connected to the (normalized, pooled) outputs of the second convolutional layer. \n",
    "\n",
    "The fourth　convolutional layer has 384 kernels of size 3 × 3 × 192 , \n",
    "and the fifth convolutional layer has 256 kernels of size 3 × 3 × 192. \n",
    "\n",
    "The fully-connected layers have 4096 neurons each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第1の畳込み層は、224×224×3の入力画像を、4×4ピクセルの幅を有する11×11×3のサイズの96個のカーネルでフィルタリングする。\n",
    "\n",
    "第2の畳込み層は、第1の畳込み層の（応答正規化され、プールされた）出力を入力として受け取り、\n",
    "サイズ5×5×48の256個のカーネルでフィルタリングする。\n",
    "\n",
    "第3、第4および第5の畳込み層は、プーリングまたは正規化層なしで互いに接続される。\n",
    "\n",
    "第3の畳込み層は、第2の畳込み層の（正規化された、プールされた）出力に接続された3×3×256のサイズの384個のカーネルを有する。\n",
    "\n",
    "第4の畳込み層は、3×3×192のサイズの384個のカーネルを有し、第5の畳込み層は、3×3×192のサイズの256個のカーネルを有する。\n",
    "全結合層にはそれぞれ4096個のニューロンがあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図3\n",
    "<img src=\"15_imgnet/img03.png\" width=\"450px\" height=\"450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reducing Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network architecture has 60 million parameters. Although the 1000 classes of ILSVRC\n",
    "make each training example impose 10 bits of constraint on the mapping from image to label, this\n",
    "turns out to be insufficient to learn so many parameters without considerable overfitting. Below, we\n",
    "describe the two primary ways in which we combat overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Data Augmentation\n",
    "\n",
    "The easiest and most common method to reduce overfitting on image data is to artificially enlarge\n",
    "the dataset using label-preserving transformations (e.g., [25, 4, 5]). \n",
    "\n",
    "We employ two distinct forms of data augmentation, both of which allow transformed images to be produced from the original images with very little computation, so the transformed images do not need to be stored on disk.\n",
    "\n",
    "In our implementation, the transformed images are generated in Python code on the CPU while the\n",
    "GPU is training on the previous batch of images. \n",
    "\n",
    "So these data augmentation schemes are, in effect, computationally free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像データのオーバーフィットを軽減する最も簡単で最も一般的な方法\n",
    "-> ラベルを維持してデータセットを拡大する。\n",
    "\n",
    "2つの異なる形式のデータ拡張を採用。両方とも、非常に少ない計算量で元の画像から変換された画像を生成可能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first form of data augmentation consists of generating image translations and horizontal reflections.\n",
    "\n",
    "We do this by extracting random 224 × 224 patches (and their horizontal reflections) from the\n",
    "256×256 images and training our network on these extracted patches.\n",
    "\n",
    "This increases the size of our training set by a factor of 2048, though the resulting training examples are, of course, highly interdependent.\n",
    "\n",
    "Without this scheme, our network suffers from substantial overfitting, which would have\n",
    "forced us to use much smaller networks.\n",
    "\n",
    "At test time, the network makes a prediction by extracting five 224 × 224 patches (the four corner patches and the center patch) as well as their horizontal reflections (hence ten patches in all), and averaging the predictions made by the network’s softmax layer on the ten patches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**データ拡張の第1**は、画像変換および水平反転を生成を行います。\n",
    "\n",
    "256×256の画像からランダムに224×224のパーツを抽出している。\n",
    "論文ではこのパーツをパッチと表現している。\n",
    "またこのパッチからさらに、水平反転してデータ拡張を実施している。\n",
    "\n",
    "→これにより、トレーニングセットのサイズは2048倍に増加。\n",
    "　データ拡張により大幅な過学習抑制が実現できている。\n",
    " \n",
    "テスト時に、ネットワークは224×224のパッチ（4つのコーナーパッチと中央パッチ）とそれらの水平反転（したがって10つのパッチ）を抽出することによって予測を行い、10個のパッチでソフトマックス層による予測を平均化します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second form of data augmentation consists of altering the intensities of the RGB channels in\n",
    "training images. \n",
    "\n",
    "Specifically, we perform PCA on the set of RGB pixel values throughout the\n",
    "ImageNet training set. \n",
    "\n",
    "To each training image, we add multiples of the found principal components,\n",
    "with magnitudes proportional to the corresponding eigenvalues times a random variable drawn from\n",
    "a Gaussian with mean zero and standard deviation 0.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**データ拡張の第2**は、トレーニング画像におけるRGBチャネルの強度変更。\n",
    "\n",
    "具体的には、ImageNetトレーニングセット全体のRGBピクセル値のセットに対してPCAを実行します。\n",
    "\n",
    "各訓練画像に、検出された主成分の倍数を加え、対応する固有値に比例する大きさと、平均ゼロおよび標準偏差0.1の\n",
    "ガウス分布から選択したランダム変数とを乗算する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore to each RGB image pixel \n",
    "$I_{xy} = [I^R_{xy}, I^G_{xy}, I^B_{xy}]^T$ we add the following quantity:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$[p_1, p_2, p_3][\\alpha_1\\lambda_1, \\alpha_2\\lambda_2, \\alpha_3\\lambda_3]^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $p_i$ and $λ_i$ are ith eigenvector and eigenvalue of the 3 × 3 covariance matrix of RGB pixel\n",
    "values, respectively, and $α_i$ is the aforementioned random variable.\n",
    "\n",
    "$p_i$, $λ_i$ は固有ベクトル、3 × 3の共分散行列の固有値、$α_i$ は前述のランダム変数\n",
    "\n",
    "Each $α_i$ is drawn only once for all the pixels of a particular training image until that image is used for training again, at which point it is re-drawn. \n",
    "\n",
    "$α_i$ はある特定の訓練画像のすべてのピクセルについて1回のみ描かれ、その画像は再び訓練に使用され、\n",
    "その時点で再描写される。\n",
    "\n",
    "This scheme approximately captures an important property of natural images, namely, that object identity is invariant to changes in the intensity and color of the illumination.\n",
    "\n",
    "This scheme reduces the top-1 error rate by over 1%.\n",
    "\n",
    "この設計は、自然画像の重要な特性、すなわち、物体の同一性が照明の強度および色の変化に対して不変であることを\n",
    "ほぼ捉えている。\n",
    "これにより、トップ1のエラー率はそれぞれ1％ 以上低下します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the predictions of many different models is a very successful way to reduce test errors [1, 3], but it appears to be too expensive for big neural networks that already take several days to train. \n",
    "\n",
    "There is, however, a very efficient version of model combination that only costs about a factor of two during training. The recently-introduced technique, called “dropout” [10], consists of setting to zero the output of each hidden neuron with probability 0.5. \n",
    "\n",
    "多くの異なるモデルの予測を組み合わせることは、テストエラーを減らすのに非常に効果的な方法[1,3]。\n",
    "大規模なニューラルネットワークで訓練するのには数日かかる。\n",
    "\n",
    "しかし、「ドロップアウト」だとトレーニングに2倍のコストしかかからない。\n",
    "\n",
    "確率0.5の各隠れニューロンの出力を0に設定することから成ります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neurons which are “dropped out” in this way do not contribute to the forward pass and do not participate in backpropagation.\n",
    "\n",
    "このように「脱落」したニューロンは、順方向パスに寄与せず、逆伝播に関与しない。\n",
    "\n",
    "So every time an input is presented, the neural network samples a different architecture, but all these architectures share weights. \n",
    "\n",
    "This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons. \n",
    "\n",
    "したがって、他のニューロンの多くの異なるランダム部分集合と関連して有用な、より堅牢な特徴を学習することが強制される。\n",
    "\n",
    "At test time, we use all the neurons but multiply their outputs by 0.5, which is a reasonable approximation to taking the geometric mean of the predictive distributions produced by the exponentially-many dropout networks. \n",
    "\n",
    "We use dropout in the first two fully-connected layers of Figure 2. Without dropout, our network exhibits substantial overfitting. Dropout roughly doubles the number of iterations required to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テスト時には、すべてのニューロンを使用しますが、指数関数的に多くのドロップアウトネットワークによって生成される予測分布の幾何平均をとるための妥当な近似値である0.5を乗算します。\n",
    "\n",
    "図2の最初の2つの完全に接続されたレイヤーでドロップアウトを使用します。<br>\n",
    "ドロップアウトがなければ、私たちのネットワークは大幅な過学習を示します。<br>\n",
    "ドロップアウトにより、収束するまで必要な反復数は約2倍になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Details of learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained our models using stochastic gradient descent\n",
    "with a batch size of 128 examples, momentum of 0.9, and\n",
    "weight decay of 0.0005. \n",
    "\n",
    "モーメンタン（Momentum）とは「運動量」のこと<br>\n",
    "地面を転がるボールが、何も力を受けない時に徐々に摩擦や空気抵抗で減速するようなイメージ\n",
    "\n",
    "We found that this small amount\n",
    "of weight decay was important for the model to learn. In\n",
    "other words, weight decay here is not merely a regularizer:\n",
    "it reduces the model’s training error.\n",
    "\n",
    "Weight decay：重みパラメータの値を小さくするように学習を行うことを目的とした手法<br>\n",
    "重みの値を小さくすることで、過学習が起きにくくなる。\n",
    "\n",
    "我々は、バッチサイズ128例、運動量0.9、および　荷重減衰(weight decay) 0.0005の確率的勾配降下法を用いてモデルを訓練した。\n",
    "\n",
    "我々は、この小さな 荷重減衰 が、モデルが学習するために重要であることを見出した。\n",
    "\n",
    "言い換えれば、荷重減衰は単なる正則化ではなく、モデルの訓練誤差を減少させます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図4\n",
    "<img src=\"15_imgnet/img04.png\" width=\"550px\" height=\"550px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialized the weights in each layer from a zero-mean Gaussian distribution with standard deviation\n",
    "0.01. \n",
    "\n",
    "各層の重みを標準偏差0.01、平均0 のガウス分布から初期化した。\n",
    "\n",
    "We initialized the neuron biases in the second, fourth, and fifth convolutional layers,\n",
    "as well as in the fully-connected hidden layers, with the constant 1.\n",
    "\n",
    "第2、第4、第5の畳み込み層、および全結合層のニューロンバイアスを定数1で初期化した。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This initialization accelerates the early stages of learning by providing the ReLUs with positive inputs.\n",
    "\n",
    "We initialized the neuron biases in the remaining layers with the constant 0.\n",
    "\n",
    "この初期化は、ReLUに正の入力を与えることによって学習の初期段階を加速する。\n",
    "\n",
    "残りの層のニューロンバイアスを定数0で初期化した。\n",
    "\n",
    "We used an equal learning rate for all layers, which we adjusted manually throughout training.\n",
    "\n",
    "全てのレイヤーで均等な学習率を使用、手動で調整。\n",
    "\n",
    "The heuristic which we followed was to divide the learning rate by 10 when the validation error rate stopped improving with the current learning rate.\n",
    "\n",
    "現在の学習率で検証エラー率が改善しなくなったときに、学習率を10で割った。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate was initialized at 0.01 and reduced three times prior to termination.\n",
    "\n",
    "We trained the network for roughly 90 cycles through the training set of 1.2 million images, which took five to six days on two NVIDIA GTX 580 3GB GPUs.\n",
    "\n",
    "学習率は0.01で初期化され、終了する前に3回減少した。\n",
    "\n",
    "我々は、NVIDIA GTX 580 3GB GPU 2台で5〜6日かかる120万枚の画像のトレーニングセットを通じて、約90サイクルのネットワークトレーニングを行いました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results on ILSVRC-2010 are summarized in Table 1. \n",
    "\n",
    "ILSVRC-2010に関する私たちの結果は、表1にまとめられています。\n",
    "\n",
    "Our network achieves top-1 and top-5 test set error rates of 37.5% and 17.0%.\n",
    "\n",
    "私たちのネットワークは、テストセットエラー率が37.5％と17.0％5のトップ1と5を達成しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performance achieved during the ILSVRC2010\n",
    "competition was 47.1% and 28.2% with an approach that averages the predictions produced from six sparse-coding models trained on different features [2], \n",
    "\n",
    "and since then the best published results are 45.7% and 25.7% with an approach that averages the predictions of two classifiers trained on Fisher Vectors (FVs) computed from two types of densely-sampled features [24].\n",
    "\n",
    "ILSVRC2010の最善のパフォーマンス\n",
    "競合は47.1％と28.2％であり、異なるフィーチャで訓練された6つの疎なコーディングモデル[2]から生成された予測を平均したアプローチで、\n",
    "\n",
    "それ以来、最高の公表された結果は、2種類の高密度サンプリングされたフィーチャ[24]から計算された<br>\n",
    "フィッシャー・ベクトル（FV）で訓練された2つの分類器の予測を平均するアプローチで45.7％と25.7％です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also entered our model in the ILSVRC-2012 competition and report our results in Table 2.\n",
    "\n",
    "私たちはまた、ILSVRC-2012競技会にモデルを公開し、結果を表2に報告しました。\n",
    "\n",
    "Since the ILSVRC-2012 test set labels are not publicly available, we cannot report test error rates for all the models that we tried.\n",
    "\n",
    "ILSVRC-2012のテストセットラベルは一般に公開されていないため、試したすべてのモデルでテストエラー率を報告することはできません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the remainder of this paragraph, we use validation and test error rates interchangeably because in our experience they do not differ by more than 0.1% (see Table 2).\n",
    "\n",
    "The CNN described in this paper achieves a top-5 error rate of 18.2%. Averaging the predictions of five similar CNNs gives an error rate of 16.4%.\n",
    "\n",
    "Training one CNN, with an extra sixth convolutional layer over the last pooling layer, to classify the entire ImageNet Fall 2011 release (15M images, 22K categories), and then “fine-tuning” it on ILSVRC-2012 gives an error rate of 16.6%.\n",
    "\n",
    "Averaging the predictions of two CNNs that were pre-trained on the entire Fall 2011 release with the aforementioned five CNNs gives an error rate of 15.3%. \n",
    "\n",
    "The second-best contest entry achieved an error rate of 26.2% with an approach that averages the predictions of several classifiers trained on FVs computed from different types of densely-sampled features [7].\n",
    "\n",
    "この段落の残りの部分では、我々の経験では0.1％以上の差がないので、検証とテストのエラーレートを同じ意味で使用しています（表2参照）。\n",
    "\n",
    "このホワイトペーパーで説明されているCNNは、エラー率上位5％を18.2％達成しています。 5つの同様のCNNの予測を平均すると、16.4％のエラー率が得られます。\n",
    "\n",
    "1つのCNNを訓練し、最後のプールレイヤーに6番目の畳み込みレイヤーを追加し、ImageNet Fall 2011全体の分類（15M画像、22Kカテゴリ）を行い、ILSVRC-2012で「微調整」すると16.6 ％。\n",
    "\n",
    "前述の5つのCNNで2011年秋の全リリースで事前にトレーニングされた2つのCNNの予測を平均すると、15.3％のエラー率が得られます。\n",
    "\n",
    "2番目に優れたコンテストエントリーでは、異なるタイプの密集サンプリングされたフィーチャから計算されたFVで訓練されたいくつかのクラシファイアの予測を平均する手法を用いて、26.2％のエラー率を達成した[7]。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we also report our error\n",
    "rates on the Fall 2009 version of\n",
    "ImageNet with 10,184 categories\n",
    "and 8.9 million images. On this\n",
    "dataset we follow the convention\n",
    "in the literature of using half of\n",
    "the images for training and half\n",
    "for testing. Since there is no established\n",
    "test set, our split necessarily\n",
    "differs from the splits used\n",
    "by previous authors, but this does\n",
    "not affect the results appreciably.\n",
    "Our top-1 and top-5 error rates\n",
    "on this dataset are 67.4% and\n",
    "40.9%, attained by the net described above but with an additional, sixth convolutional layer over the\n",
    "last pooling layer. The best published results on this dataset are 78.1% and 60.9% [19]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図5\n",
    "<img src=\"15_imgnet/img05.png\" width=\"350px\" height=\"350px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図6\n",
    "<img src=\"15_imgnet/img06.png\" width=\"350px\" height=\"350px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Qualitative Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 3 shows the convolutional kernels learned by the network’s two data-connected layers. The\n",
    "network has learned a variety of frequency- and orientation-selective kernels, as well as various colored\n",
    "blobs. Notice the specialization exhibited by the two GPUs, a result of the restricted connectivity\n",
    "described in Section 3.5. The kernels on GPU 1 are largely color-agnostic, while the kernels\n",
    "on on GPU 2 are largely color-specific. This kind of specialization occurs during every run and is\n",
    "independent of any particular random weight initialization (modulo a renumbering of the GPUs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図7\n",
    "<img src=\"15_imgnet/img07.png\" width=\"550px\" height=\"550px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the left panel of Figure 4 we qualitatively assess what the network has learned by computing its\n",
    "top-5 predictions on eight test images. Notice that even off-center objects, such as the mite in the\n",
    "top-left, can be recognized by the net. Most of the top-5 labels appear reasonable. For example,\n",
    "only other types of cat are considered plausible labels for the leopard. In some cases (grille, cherry)\n",
    "there is genuine ambiguity about the intended focus of the photograph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to probe the network’s visual knowledge is to consider the feature activations induced\n",
    "by an image at the last, 4096-dimensional hidden layer. If two images produce feature activation\n",
    "vectors with a small Euclidean separation, we can say that the higher levels of the neural network\n",
    "consider them to be similar. Figure 4 shows five images from the test set and the six images from\n",
    "the training set that are most similar to each of them according to this measure. Notice that at the\n",
    "pixel level, the retrieved training images are generally not close in L2 to the query images in the first\n",
    "column. For example, the retrieved dogs and elephants appear in a variety of poses. We present the\n",
    "results for many more test images in the supplementary material.\n",
    "\n",
    "Computing similarity by using Euclidean distance between two 4096-dimensional, real-valued vectors\n",
    "is inefficient, but it could be made efficient by training an auto-encoder to compress these vectors\n",
    "to short binary codes. This should produce a much better image retrieval method than applying autoencoders\n",
    "to the raw pixels [14], which does not make use of image labels and hence has a tendency\n",
    "to retrieve images with similar patterns of edges, whether or not they are semantically similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results show that a large, deep convolutional neural network is capable of achieving recordbreaking\n",
    "results on a highly challenging dataset using purely supervised learning. It is notable\n",
    "that our network’s performance degrades if a single convolutional layer is removed. For example,\n",
    "removing any of the middle layers results in a loss of about 2% for the top-1 performance of the\n",
    "network. So the depth really is important for achieving our results.\n",
    "\n",
    "To simplify our experiments, we did not use any unsupervised pre-training even though we expect\n",
    "that it will help, especially if we obtain enough computational power to significantly increase the\n",
    "size of the network without obtaining a corresponding increase in the amount of labeled data. Thus\n",
    "far, our results have improved as we have made our network larger and trained it longer but we still\n",
    "have many orders of magnitude to go in order to match the infero-temporal pathway of the human\n",
    "visual system. Ultimately we would like to use very large and deep convolutional nets on video\n",
    "sequences where the temporal structure provides very helpful information that is missing or far less\n",
    "obvious in static images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "\n",
    "[1] ImageNet Classification with Deep Convolutional Neural Networks<br>\n",
    "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "畳み込みニューラルネットワークの最新研究動向 (〜2017)<br>\n",
    "https://qiita.com/yu4u/items/7e93c454c9410c4b5427"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
